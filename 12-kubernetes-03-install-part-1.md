# Домашнее задание к занятию "12.3 Развертывание кластера на собственных серверах, лекция 1"
Поработав с персональным кластером, можно заняться проектами. Вам пришла задача подготовить кластер под новый проект.

## Задание 1: Описать требования к кластеру
Сначала проекту необходимо определить требуемые ресурсы. Известно, что проекту нужны база данных, система кеширования, а само приложение состоит из бекенда и фронтенда. Опишите, какие ресурсы нужны, если известно:

* База данных должна быть отказоустойчивой. Потребляет 4 ГБ ОЗУ в работе, 1 ядро. 3 копии.
* Кэш должен быть отказоустойчивый. Потребляет 4 ГБ ОЗУ в работе, 1 ядро. 3 копии.
* Фронтенд обрабатывает внешние запросы быстро, отдавая статику. Потребляет не более 50 МБ ОЗУ на каждый экземпляр, 0.2 ядра. 5 копий.
* Бекенд потребляет 600 МБ ОЗУ и по 1 ядру на копию. 10 копий.

## Как оформить ДЗ?

Выполненное домашнее задание пришлите ссылкой на .md-файл в вашем репозитории.

План расчета
1. Сначала сделайте расчет всех необходимых ресурсов.
2. Затем прикиньте количество рабочих нод, которые справятся с такой нагрузкой.
3. Добавьте к полученным цифрам запас, который учитывает выход из строя как минимум одной ноды.
4. Добавьте служебные ресурсы к нодам. Помните, что для разных типов нод требовния к ресурсам разные.
5. Рассчитайте итоговые цифры.
6. В результате должно быть указано количество нод и их параметры.


Параметры кластера

Наименование  |Одна нода|	Кол-во нод|	Всего	|Резерв|	Итого
--|---|---|---|---|--

	Диск	|||||			232,5
БД|	20|	3|	60	|1,5|	90
Кэш	|20	|3|	60|	1,5	|90
Фронт|	1	|5|	5|	1,5|	7,5
Бэк	|3|	10	|30	|1,5|	45
	Ядра ЦПУ|||||				25,5
БД	|1|	3|	3	|1,5|	4,5
Кэш	|1|	3	|3	|1,5|	4,5
Фронт	|0,2|	5|	1	|1,5|	1,5
Бэк	|1|	10	|10	|1,5|	15
	ОЗУ	|||||			45,375
БД|	4	|3|	12	|1,5|	18
Кэш	|4	|3	|12	|1,5|	18
Фронт	|0,05	|5	|0,25|	1,5	|0,375
Бэк|	0,6|	10|	6	|1,5|	9


Системные параметры

| Наименование  | Одна нода | Кол-во нод | Всего |
|---------------|-----------|------------|-------|
|  Диск             |       |            | 370   |
| Control plane | 50        | 1          | 50    |
| Worker node   | 100       | 3          | 300   |
| ОС            | 20        | 1          | 20    |
| Ядра ЦПУ              |   |            | 6     |
| Control plane | 2         | 1          | 7     |
| Worker node   | 1         | 3          | 3     |
| ОС            | 2         | 1          | 2     |
| ОЗУ              |        |            | 7     |
| Control plane | 2         | 1          | 2     |
| Worker node   | 1         | 3          | 3     |
| ОС            | 2         | 1          | 2     |



```
  Итого:
  Диск - 602,5 (Т.е. закупать надо будет 2 диска по 1Тб для RAID)
  ЦПУ - 32,5 ядер (Подобрать процессоры где будет не меньше такого кол-ва ядер. Скорее всего на 64 ядра)
  ОЗУ - 52,375 (Т.е. закупать 64Гб)

  В итоге будет 4 ноды:
  Control     - 	Диск: 100Гб
  		ЦПУ: 4 ядра
  		ОЗУ: 4 Гб
  Worker (3 ноды) - Диск: по 300Гб
  		      ЦПУ: по 20 ядер
  		      ОЗУ: по 20 Гб
```

---

cd kubernetes-for-beginners/99-misc
./list-vms.sh
./create-vms.sh
ssh yc-user@ip-адрес
на контрол плане:
sudo su
выполнить все из 15-install/10-kubeadm/10-kubeadm-control-plane.md ## Установка минимального набора ПО
ip a
копируем внутренний ip и вставляем в --apiserver-advertise-address=
а сюда вставляем внешний ip --apiserver-cert-extra-sans=
запускаем # Инициализация кластера
выполнить ### Исправление ошибки
потом опять запускаем # Инициализация кластера
копируем команду kubeadm
выполнить ### Доступ к управлению кластером под root
get nodes
exit - выходим из рута
kubectl get nodes
выполнить ### Доступ к управлению кластером под root
kubectl get nodes
kubectl describe nodes master1(master1 - это имя_ноды) | grep KubeletNotReady
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
watch kubectl get nodes


ssh yc-user@ip-адрес worker-node
на воркер ноде:
15-install/10-kubeadm/20-kubeadm-worker-node.md
sudo su
выполнить ## Установка минимального набора ПО
и далее по файлу 20-kubeadm-worker-node.md
токен печатается при создании контрол ноды

в файле ./kube/config копируем секцию cluster для создания нового подключения
параметры подключения к контрол ноде берем из /kube/config контрол ноды
kubectl get pod -o wide
добавляем второй неймспейс
kubectl get ns
kubectl get pod -o wide --all-namespaces (kubectl get pod -o wide -A)
kubectl create deployment nginx --namespace default --image=nginx:latest --replicas=2
kubectl get pod -o wide -A
в context меняем namespace на default
kubeadm token list
